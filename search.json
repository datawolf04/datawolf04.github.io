[
  {
    "objectID": "posts/olympic240805/olympicMedalData.html",
    "href": "posts/olympic240805/olympicMedalData.html",
    "title": "Olympic countries: Fun facts and visualizations",
    "section": "",
    "text": "This week we’re exploring Olympics data!\nThe data this week comes from the RGriffin Kaggle dataset: 120 years of Olympic history: athletes and results, basic bio data on athletes and medal results from Athens 1896 to Rio 2016.\nlibrary(tidyverse)\nlibrary(ggtext)\nlibrary(showtext)\nlibrary(janitor)\nlibrary(ggdist)\nlibrary(ggrepel)\nlibrary(scales)\nlibrary(paletteer)\nlibrary(gt)\n\n\nload('olympicMedalResults.RData')"
  },
  {
    "objectID": "posts/olympic240805/olympicMedalData.html#data-in-this-dataset",
    "href": "posts/olympic240805/olympicMedalData.html#data-in-this-dataset",
    "title": "Olympic countries: Fun facts and visualizations",
    "section": "Data in this dataset:",
    "text": "Data in this dataset:\nFirst, I am planning on analyzing the summer and winter olympics separately. Furthermore, it occurs to me that the year in this dataset should be treated as a factor variable, rather than a numeric variable. This will help if (for example) the olympics were skipped or postponed, and it will definitely help when the winter games were shifted by 2 years so that the intervals between games are uniform.\n\ncountryParticipation &lt;- olympics |&gt; count(noc) |&gt;\n  arrange(desc(n)) \n\nMy plan is to plot out participation, medal count, and gold medal count by country NOC region. The reason for this due to the fact that there are an obnoxious number of “teams” (1184) already in this dataset. Also, I don’t think that we want to track the results of “Osborne Swimming Club, Manchester” or “Phalainis ton Thorichtou ‘Psara’-3” on the same level as Spain, China, Brazil, USA, etc. As it stands, there are plenty of countries to keep track of! As it is, there are 230 national olympic committeees. For fun, let’s look at the least frequent participants.\n\n\n\n\n\n\n\n\n\n\n\n\nOlympic participant breakdown\n\n\n10 lowest participating countries\n\n\nCountry code\nParticipant #\n\n\n\n\nYAR\n11\n\n\nBRU\n10\n\n\nTLS\n9\n\n\nKOS\n8\n\n\nTUV\n7\n\n\nYMD\n5\n\n\nSSD\n3\n\n\nNBO\n2\n\n\nUNK\n2\n\n\nNFL\n1\n\n\n\n#TidyTuesday • Source: RGriffin Kaggle dataset\n\n\n\n\n\n\n\n\n\nAside: Ummm…\nWhat is NFL and who is the lone participant from there?\n\nolympics |&gt; filter(noc==\"NFL\") |&gt; glimpse()\n\nRows: 1\nColumns: 15\n$ id     &lt;dbl&gt; 36547\n$ name   &lt;chr&gt; \"Robert Arthur \\\"Bob\\\" Fowler\"\n$ sex    &lt;chr&gt; \"M\"\n$ age    &lt;dbl&gt; 21\n$ height &lt;dbl&gt; 170\n$ weight &lt;dbl&gt; 57\n$ team   &lt;chr&gt; \"Newfoundland\"\n$ noc    &lt;chr&gt; \"NFL\"\n$ games  &lt;chr&gt; \"1904 Summer\"\n$ year   &lt;dbl&gt; 1904\n$ season &lt;chr&gt; \"Summer\"\n$ city   &lt;chr&gt; \"St. Louis\"\n$ sport  &lt;chr&gt; \"Athletics\"\n$ event  &lt;chr&gt; \"Athletics Men's Marathon\"\n$ medal  &lt;chr&gt; NA\n\n\nOk, wait a minute. So the Canadians don’t want to accept this guy? Was Newfoundland it’s own country back then?\n(Feverish googling occurs)\nWell, I’ll be darned, it was! According to Wikipedia:\n\nUntil 1949, the Dominion of Newfoundland was a separate dominion in the British Empire. In 1933, the House of Assembly of the self-governing dominion voted to dissolve itself and to hand over administration of Newfoundland and Labrador to the British-appointed Commission of Government. This followed the suffering caused by the Great Depression and Newfoundland’s participation in the First World War. On March 31, 1949, it became the 10th and most recent province to join the Canadian Confederation as “Newfoundland”. On December 6, 2001, the Constitution of Canada was amended to change the province’s name from “Newfoundland” to “Newfoundland and Labrador”.\n\nHuh…I learn something new every day! And here I thought it was because most Canadians think (insert Newfie joke here)."
  },
  {
    "objectID": "posts/olympic240805/olympicMedalData.html#back-to-the-show",
    "href": "posts/olympic240805/olympicMedalData.html#back-to-the-show",
    "title": "Olympic countries: Fun facts and visualizations",
    "section": "Back to the show",
    "text": "Back to the show\nI think that I should create a function that sorts the participation data (and later, the medalist data) by country and year. Also, since 230 lines would make the graph look like messy spaghetti, I think it would be best to limit the results to the top 10 countries by the metric being plotted (participation, number medalists, number gold medalists).\n\ncountByCountryYearSzn = function(dat,top=TRUE){\n  cbcys &lt;- dat |&gt; group_by(across(all_of(c(\"season\",\"noc\",\"year\")))) |&gt;\n      summarize(\n        pcount = n()\n      ) |&gt;\n    mutate(\n      cumCount = cumsum(pcount)\n    )\n  \n  countryTotal &lt;- dat |&gt; count(noc) |&gt;\n    arrange(desc(n))\n  \n  nC = nrow(countryTotal)\n  \n  if(top){\n    cKeep = countryTotal[1:10, ]\n  }else{\n    cKeep = countryTotal[(nC-10):nC, ]\n  }\n  \n  out &lt;- cbcys |&gt; inner_join(cKeep,join_by(noc))\n  \n  out$noc = factor(out$noc, levels = cKeep$noc)\n\n  return(out)\n}\n\nNow, let’s make a graph of olympic participation:\n\npart = countByCountryYearSzn(olympics,top=TRUE)\n\n`summarise()` has grouped output by 'season', 'noc'. You can override using the\n`.groups` argument.\n\nggplot(part, aes(x=year, y=cumCount, color=noc)) +\n  geom_step() + facet_wrap(~ season, ncol=1, scales=\"free_y\") + \n  theme_simple() + theme(\n    plot.background = element_rect(fill=grn,color=grn),\n    panel.background = element_rect(fill=ltGrn,color=ltGrn),\n    legend.key = element_rect(fill=ltGrn,color=ltGrn),\n    legend.background = element_rect(fill=ltGrn,color=ltGrn)\n  ) +\n  labs(\n    title = \"Olympic participation by country (top 10)\",\n    x = \"Year of Olympic Games\",\n    y = \"Cumulative participant count\",\n    caption = caption_text,\n    color = \"Nation\"\n  ) +\n  scale_color_paletteer_d(col_pal_dis_long)\n\n\n\n\n\n\n\n\nThe medal breakdown\n\nolyMedal &lt;- olympics |&gt; filter(!is.na(medal))\n\nmedalists = countByCountryYearSzn(olyMedal,top=TRUE)\n\n`summarise()` has grouped output by 'season', 'noc'. You can override using the\n`.groups` argument.\n\nggplot(medalists, aes(x=year,y=cumCount,color=noc)) +\n  geom_step() + facet_wrap(~ season, ncol=1, scales=\"free_y\") + \n  theme_simple() + theme(\n    plot.background = element_rect(fill=silver,color=silver),\n    panel.background = element_rect(fill=ltSilver,color=ltSilver),\n    legend.key = element_rect(fill=ltSilver,color=ltSilver),\n    legend.background = element_rect(fill=ltSilver,color=ltSilver)\n  ) +\n  labs(\n    title=\"Olympic medalists by country (top 10)\",\n    x = \"Year of Olympic Games\",\n    y = \"Cumulative medal count\",\n    caption = caption_text,\n    color = \"Nation\"\n  ) +\n  scale_color_paletteer_d(col_pal_dis_long)\n\n\n\n\n\n\n\n\nI will note that URS is the Soviet Union, and RUS is Russia. Clearly that country’s 20th century history means that it is not treated as continuous by the Olympic Federation. It reminds me of when I was a kid watching Where in the World is Carmen Sandiego and all of a sudden there were new countries on the European maps. &lt;/nostalgia trip&gt;\nBut you can totally see the rivalry fueled by the Cold War when you look at the “slope” of the summer olympic medal accumulation graph and compare the Soviets (and later, the Russians) to the Americans.\n\nolyGold &lt;- olympics |&gt; filter(medal==\"Gold\")\n\ngoldMetals = countByCountryYearSzn(olyGold,top=TRUE)\n\n`summarise()` has grouped output by 'season', 'noc'. You can override using the\n`.groups` argument.\n\nggplot(goldMetals, aes(x=year,y=cumCount,color=noc)) +\n  geom_step() + facet_wrap(~ season, ncol = 1, scales=\"free_y\") + \n  theme_simple() + theme(\n    plot.background = element_rect(fill=gold,color=gold),\n    panel.background = element_rect(fill=ltGold,color=ltGold),\n    legend.key = element_rect(fill=ltGold,color=ltGold),\n    legend.background = element_rect(fill=ltGold,color=ltGold)\n  ) +\n  labs(\n    title=\"Olympic Golds by country (top 10)\",\n    x = \"Year of Olympic Games\",\n    y = \"Cumulative gold metal count\",\n    caption = caption_text,\n    color = \"Nation\"\n  ) +\n  scale_color_paletteer_d(col_pal_dis_long)"
  },
  {
    "objectID": "posts/tt20240903/stackOverflow.html",
    "href": "posts/tt20240903/stackOverflow.html",
    "title": "Stack Overflow Annual Developer Survey",
    "section": "",
    "text": "This week’s dataset is derived from the 2024 Stack Overflow Annual Developer Survey. Conducted in May 2024, the survey gathered responses from over 65,000 developers.\nGiven that one of the big trends being discussed in the world right now is working from home vs. going back to the office, I was wondering if there were any trends in this data as to who is working at home. Note, that this dataset did not include gender or race data. In particular, I’m considering 3 questions:\nload(\"soSurvey.RData\")\nsource('helperFunctions.R')\nlibrary(tidyverse)\nlibrary(paletteer)\n\npersonalDataCats = c('main_branch','age','remote_work','ed_level','years_code',\n                     'years_code_pro','dev_type','org_size','country','currency',\n                     'comp_total')\n\npersonData &lt;- stackoverflow_survey_single_response |&gt;\n  select(all_of(personalDataCats)) |&gt;\n  filter(!is.na(remote_work))\n\ncwRemote = qname_levels_single_response_crosswalk |&gt;\n  filter(qname=='remote_work')\n\ncwRemote$label = c(\"Hybrid\",\"In-person\",\"Remote\")\nremoteLevels = c(\"In-person\",\"Hybrid\",\"Remote\")\n\npersonData &lt;- left_join(personData,cwRemote,by=join_by(remote_work == level)) |&gt;\n  mutate(remote_work = factor(label,levels=remoteLevels)) |&gt; \n  select(!qname & !label)"
  },
  {
    "objectID": "posts/tt20240903/stackOverflow.html#work-type-and-education-level",
    "href": "posts/tt20240903/stackOverflow.html#work-type-and-education-level",
    "title": "Stack Overflow Annual Developer Survey",
    "section": "Work type and education level",
    "text": "Work type and education level\n\n\n\n\n\n\n\n\n\nThis graph shows the number of responses as well as the level of education that each user has. We can see that most users of Stack Overflow have at least 4-year college degree. This would seem to be in line with the general profile of someone working in a STEM field. It is somewhat difficult to glean what, if any, role there is between work location and education level.\n\n\n\n\n\n\n\n\n\nNext, we consider the same data, but we are highlighting the counts based on work location. This plot shows that a large fraction of Stack Overflow users are working in hybrid or remote locations.\nIn the following two plots, I’ll show the same information as above, except as percentages rather than raw counts.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNeither of these suggests that education level would predict work location for stack overflow users."
  },
  {
    "objectID": "posts/tt20240903/stackOverflow.html#work-type-and-age",
    "href": "posts/tt20240903/stackOverflow.html#work-type-and-age",
    "title": "Stack Overflow Annual Developer Survey",
    "section": "Work type and age",
    "text": "Work type and age\n\n\n\n\n\n\n\n\n\nThe above plot looks at the number of responses by age. We can see clearly that most users are between ages 25 and 44.\n\n\n\n\n\n\n\n\n\nHere we see that the age group from 18-24 is working in-person much more than they work hybrid/remote. As this group is college-aged, and most colleges and universities are back to in-person instruction, this could suggest that students who are learning to code at university are using Stack Overflow for help (as this college professor hopes they would). As users age, they tend to find more hybrid/remote roles."
  },
  {
    "objectID": "posts/tt20240903/stackOverflow.html#work-type-and-experience-coding",
    "href": "posts/tt20240903/stackOverflow.html#work-type-and-experience-coding",
    "title": "Stack Overflow Annual Developer Survey",
    "section": "Work type and Experience coding",
    "text": "Work type and Experience coding\n\n\n\n\n\n\n\n\n\nHere we see a possible echo of the college idea that I raised above. However, I see a few other things as well.\n\nThere seems to be a significant shift that as people gain experience coding, they move from in-person roles to hybrid/remote roles.\nAs you do something for a longer time, you tend to estimate more. All of the responses seem to favor numbers ending in 0 or 5, especially once you get past 10 years. So if someone has 22 years of experience, they will probably round down to 20, or up to 25, rather than stating 20 exactly."
  },
  {
    "objectID": "posts/tt20240813/worldFairs.html",
    "href": "posts/tt20240813/worldFairs.html",
    "title": "World’s Fairs",
    "section": "",
    "text": "This week’s #TidyTuesday is all about World’s Fairs, and I have always wanted to figure out how to make a visualization with a map. So this is the week that I try that out. In addition to the data from Wikipedia in the given dataset, I have also downloaded S. Altan’s Kaggle Dataset: Countries by Continent.\nlibrary(tidyverse)\nlibrary(gt)\nlibrary(rworldmap)\nlibrary(paletteer)\nlibrary(RColorBrewer)\n\nload('worldsFairs.Rdata')\nsource('helperFunctions.R')"
  },
  {
    "objectID": "posts/tt20240813/worldFairs.html#plan-and-data-cleaning",
    "href": "posts/tt20240813/worldFairs.html#plan-and-data-cleaning",
    "title": "World’s Fairs",
    "section": "Plan and data cleaning",
    "text": "Plan and data cleaning\nMy plan is to create visualizations that look at different world’s fair properties by continent and country. I will clean the data a bit, and then generate some tables.\n\nContinental breakdown of World’s fair\n\nfairData = left_join(worlds_fairs,countryContinent, by='country')\n\n# Replace missing continents by hand\nmissing = c(\"Europe\",\"Oceania\",\"Europe\",\"Europe\",\"Asia\")\nfairData$continent[is.na(fairData$continent)] = missing\n\nfairDataByContinent = fairData |&gt;\n  mutate(\n    duration = calcFLMV(start_month, start_year, end_month, end_year) \n  ) |&gt; group_by(continent) |&gt; \n  summarize(\n    N = n(),\n    across(where(is.numeric), ~ mean(.x,na.rm=TRUE))\n  ) |&gt;\n  select(-c('start_month','start_year','end_month','end_year'))\n\n\n\n\n\n\n  \n    \n      World's Fair by the numbers\n    \n    \n      Average properties by host continent\n    \n    \n      Continent\n      Number Hosted\n      Visitors (M)\n      Avg. cost (M$)\n      Land area (hectares)\n      Number of attending countries\n      Fair duration (mo)\n    \n  \n  \n    Asia\n11\n26.0\n1,999.7\n169.4\n103.5\n5.1\n    Europe\n43\n12.7\n547.8\n77.1\n42.7\n5.2\n    North America\n14\n20.9\n281.4\n144.2\n29.8\n8.9\n    Oceania\n2\n9.9\n320.5\n32.5\n34.5\n7.0\n  \n  \n    \n      #TidyTuesday • Sources: Wikipedia and S. Altan Kaggle dataset\n\n    \n  \n  \n    \n       Missing data was ignored from averages taken.\n    \n  \n\n\n\n\n\n\nCountry breakdown of world’s fair\n\nfairDataByCountry = fairData |&gt;\n  mutate(\n    duration = calcFLMV(start_month, start_year, end_month, end_year) \n  ) |&gt; group_by(country) |&gt; \n  summarize(\n    N = n(),\n    across(where(is.numeric), ~ mean(.x,na.rm=TRUE))\n  ) |&gt;\n  select(-c('start_month','start_year','end_month','end_year'))\n\n\n\n\n\n\n  \n    \n      World's Fair by the numbers\n    \n    \n      Average properties by host country\n    \n    \n      Country\n      Number Hosted\n      Visitors (M)\n      Fair cost (M$)\n      Land area (hectares)\n      Number of attending countries\n      Fair duration (mo)\n    \n  \n  \n    Australia\n1\n18.5\n625.0\n40.0\n36.0\n7.0\n    Austria-Hungary\n1\n7.2\n95.0\n233.0\n35.0\n6.0\n    Belgium\n7\n16.4\n558.8\n117.7\n26.9\n6.7\n    Bulgaria\n3\n1.0\n—\n5.8\n40.5\n1.7\n    Canada\n2\n36.2\n371.0\n217.5\n58.5\n6.5\n    Colony of Victoria\n1\n1.3\n16.0\n25.0\n33.0\n7.0\n    Finland\n1\n—\n—\n0.1\n25.0\n1.0\n    France\n9\n21.6\n556.0\n67.9\n34.5\n5.4\n    Germany\n1\n18.1\n3,400.0\n160.0\n155.0\n5.0\n    Haiti\n1\n—\n—\n24.0\n15.0\n7.0\n    Hungary\n1\n1.9\n—\n35.0\n52.0\n2.0\n    Israel\n2\n—\n—\n29.8\n13.0\n2.0\n    Italy\n8\n7.7\n26.0\n68.3\n44.1\n5.5\n    Japan\n4\n27.5\n30.0\n175.7\n69.8\n7.0\n    Kazakhstan\n1\n4.1\n—\n25.0\n115.0\n4.0\n    People's Republic of China\n1\n73.1\n4,200.0\n523.0\n192.0\n6.0\n    Portugal\n1\n10.1\n—\n50.0\n143.0\n5.0\n    South Korea\n2\n11.3\n1,769.0\n57.5\n122.0\n4.0\n    Spain\n4\n13.9\n21.0\n101.2\n68.8\n7.2\n    Sweden\n3\n—\n—\n0.5\n18.3\n2.3\n    United Arab Emirates\n1\n24.1\n—\n438.0\n192.0\n6.0\n    United Kingdom\n2\n6.0\n83.5\n12.5\n32.0\n7.0\n    United States\n11\n18.2\n265.1\n141.8\n25.6\n9.5\n    West Germany\n2\n2.1\n—\n51.6\n24.5\n4.0\n  \n  \n    \n      #TidyTuesday • Sources: Wikipedia and S. Altan Kaggle dataset\n\n    \n  \n  \n    \n       Missing data was ignored from averages taken. Blank values imply no data exists for that country and quantity."
  },
  {
    "objectID": "posts/tt20240813/worldFairs.html#putting-this-on-a-map",
    "href": "posts/tt20240813/worldFairs.html#putting-this-on-a-map",
    "title": "World’s Fairs",
    "section": "Putting this on a map",
    "text": "Putting this on a map\n\nworldFairMap &lt;- joinCountryData2Map(fairDataByCountry, joinCode = 'NAME',\n                                    nameJoinColumn = \"country\")\n\n21 codes from your data successfully matched countries in the map\n3 codes from your data failed to match with a country code in the map\n222 codes from the map weren't represented in your data\n\npar(mar=c(0,0.1,2,0.1),xaxs=\"i\",yaxs=\"i\",cex.main=2)\nattendMap = mapCountryData(worldFairMap, addLegend=FALSE,\n                           nameColumnToPlot = 'attending_countries',\n                           catMethod = \"pretty\",\n                           colourPalette = brewer.pal(10,\"Purples\"),\n                           oceanCol='lightblue',missingCountryCol = 'white',\n                           mapTitle=\"Country attendance\")\n\nYou asked for 7 categories, 10 were used due to pretty() classification\n\n\nWarning in brewer.pal(10, \"Purples\"): n too large, allowed maximum for palette Purples is 9\nReturning the palette you asked for with that many colors\n\n\nWarning in rwmGetColours(colourPalette, numColours): 9 colours specified and 10\nrequired, using interpolation to calculate colours\n\ndo.call(addMapLegend, c(attendMap, legendWidth=0.5, legendMar=5))\n\n\n\n\n\n\n\ncostMap = mapCountryData(worldFairMap, nameColumnToPlot = 'cost',\n               catMethod = \"pretty\", addLegend=FALSE,\n               colourPalette = brewer.pal(10,\"Greens\"),\n               oceanCol='lightblue',missingCountryCol = 'white',\n               mapTitle=\"Cost in Millions\")\n\nYou asked for 7 categories, 9 were used due to pretty() classification\n\n\nWarning in brewer.pal(10, \"Greens\"): n too large, allowed maximum for palette Greens is 9\nReturning the palette you asked for with that many colors\n\ndo.call(addMapLegend, c(costMap, legendWidth=0.5, legendMar=5))\n\n\n\n\n\n\n\nareaMap = mapCountryData(worldFairMap, nameColumnToPlot = 'area',\n               catMethod = \"pretty\", addLegend=FALSE,\n               colourPalette = brewer.pal(10,\"PuBuGn\"),\n               oceanCol='lightblue',missingCountryCol = 'white',\n               mapTitle=\"Size of fairgrounds\")\n\nYou asked for 7 categories, 6 were used due to pretty() classification\n\n\nWarning in brewer.pal(10, \"PuBuGn\"): n too large, allowed maximum for palette PuBuGn is 9\nReturning the palette you asked for with that many colors\n\n\nWarning in rwmGetColours(colourPalette, numColours): 9 colours specified and 6\nrequired, using interpolation to calculate colours\n\ndo.call(addMapLegend, c(areaMap, legendWidth=0.5, legendMar=5))\n\n\n\n\n\n\n\nvisitMap = mapCountryData(worldFairMap, nameColumnToPlot = 'visitors',\n               catMethod = \"pretty\", addLegend=FALSE,\n               colourPalette = brewer.pal(10,\"OrRd\"),\n               oceanCol='lightblue',missingCountryCol = 'white',\n               mapTitle=\"Fair Visitors by the Million\")\n\nYou asked for 7 categories, 8 were used due to pretty() classification\n\n\nWarning in brewer.pal(10, \"OrRd\"): n too large, allowed maximum for palette OrRd is 9\nReturning the palette you asked for with that many colors\n\n\nWarning in rwmGetColours(colourPalette, numColours): 9 colours specified and 8\nrequired, using interpolation to calculate colours\n\ndo.call(addMapLegend, c(visitMap, legendWidth=0.5, legendMar=5))\n\n\n\n\n\n\n\ntimeMap = mapCountryData(worldFairMap, nameColumnToPlot = 'duration',\n               catMethod = \"pretty\", addLegend=FALSE,\n               colourPalette = brewer.pal(10,\"OrRd\"),\n               oceanCol='lightblue',missingCountryCol = 'white',\n               mapTitle=\"Length of the World's Fair\")\n\nYou asked for 7 categories, 9 were used due to pretty() classification\n\n\nWarning in brewer.pal(10, \"OrRd\"): n too large, allowed maximum for palette OrRd is 9\nReturning the palette you asked for with that many colors\n\ndo.call(addMapLegend, c(timeMap, legendWidth=0.5, legendMar=5))"
  },
  {
    "objectID": "posts/tt20240819/englishMonarchy.html",
    "href": "posts/tt20240819/englishMonarchy.html",
    "title": "English Monarchy",
    "section": "",
    "text": "This week’s #TidyTuesday is all about exploring English Monarchs and Marriages!\nThe data was scraped from Ian Visits by f. hull, who also curated this week’s post!"
  },
  {
    "objectID": "posts/tt20240819/englishMonarchy.html#data-cleaning",
    "href": "posts/tt20240819/englishMonarchy.html#data-cleaning",
    "title": "English Monarchy",
    "section": "Data cleaning",
    "text": "Data cleaning\nAs one might expect from a dataset that has been scraped from a webpage written by a London history/culture type, there is some inconsistency in how the data has been cataloged. And as the prompt for the week says, some of the early data is “fuzzy”. So I want to clean it up. Here is a quick look at the data:\n\n\nRows: 84\nColumns: 5\n$ sovereign_name   &lt;chr&gt; \"Æthelwulf\", \"Æthelwulf\", \"Æthelbald\", \"Æthelberht\", …\n$ sovereign_age    &lt;chr&gt; \"?\", \"50(?)\", \"24\", \"–\", \"?\", \"19\", \"19\", \"28\", \"31\",…\n$ consort_name     &lt;chr&gt; \"Osburh\", \"Judith of Flanders\", \"Judith of Flanders\",…\n$ consort_age      &lt;chr&gt; \"?\", \"12\", \"14\", \"–\", \"?\", \"16\", \"?\", \"?\", \"?\", \"–\", …\n$ year_of_marriage &lt;chr&gt; \"851(?)\", \"856\", \"858\", \"–\", \"?\", \"868\", \"893\", \"902\"…\n\n\nAs you can see above, uncertainty in the dataset is often indicated with ? or - being included in the string. Ultimately I want to work with numeric variables, for things like age and year, so I’m going to need to clean them up. I also want to track whether a date or age is uncertain, so I am going to create columns tracking that information. All of the queens in the dataset are named Mary, Elizabeth, Anne, or Victoria. Using str_detect and some OR logic, I believe that I have caught all of them.\n\n\nRows: 84\nColumns: 14\n$ sovereign_name       &lt;chr&gt; \"Æthelwulf\", \"Æthelwulf\", \"Æthelbald\", \"Æthelberh…\n$ sovereign_age        &lt;dbl&gt; NA, 50, 24, NA, NA, 19, 19, 28, 31, NA, NA, 22, N…\n$ consort_name         &lt;chr&gt; \"Osburh\", \"Judith of Flanders\", \"Judith of Flande…\n$ consort_age          &lt;dbl&gt; NA, 12, 14, NA, NA, 16, NA, NA, NA, NA, NA, NA, N…\n$ year_of_marriage     &lt;dbl&gt; 851, 856, 858, NA, NA, 868, 893, 902, 905, NA, NA…\n$ sovereign_age_unc    &lt;lgl&gt; TRUE, TRUE, FALSE, FALSE, TRUE, FALSE, FALSE, FAL…\n$ consort_age_unc      &lt;lgl&gt; TRUE, FALSE, FALSE, FALSE, TRUE, FALSE, TRUE, TRU…\n$ year_of_marriage_unc &lt;lgl&gt; TRUE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FA…\n$ any_unc              &lt;lgl&gt; TRUE, TRUE, FALSE, FALSE, TRUE, FALSE, TRUE, TRUE…\n$ sovereign_type       &lt;fct&gt; King, King, King, King, King, King, King, King, K…\n$ age_diff_pos         &lt;lgl&gt; NA, TRUE, TRUE, NA, NA, TRUE, NA, NA, NA, NA, NA,…\n$ age_diff             &lt;dbl&gt; NA, 38, 10, NA, NA, 3, NA, NA, NA, NA, NA, NA, NA…\n$ min_age              &lt;dbl&gt; NA, 12, 14, NA, NA, 16, NA, NA, NA, NA, NA, NA, N…\n$ max_age              &lt;dbl&gt; NA, 50, 24, NA, NA, 19, NA, NA, NA, NA, NA, NA, N…"
  },
  {
    "objectID": "posts/tt20240819/englishMonarchy.html#plan",
    "href": "posts/tt20240819/englishMonarchy.html#plan",
    "title": "English Monarchy",
    "section": "Plan",
    "text": "Plan\nMake a plot with sovereign age and consort age vs. the year of marriage. On this plot I’m going to highlight:\n\nThe age gap. If both sovereign and consort have the age recorded in the data, there will be a vertical line drawn from the younger to the elder.\nThe gender of the sovereign. Kings are light blue and queens are pink.\nMarriages with uncertain data (either in the year or the ages of those involved) are more transparent so that they stand out less."
  },
  {
    "objectID": "posts/tt20240819/englishMonarchy.html#one-more-visualization",
    "href": "posts/tt20240819/englishMonarchy.html#one-more-visualization",
    "title": "English Monarchy",
    "section": "One more visualization",
    "text": "One more visualization\nPairs plots are fun, and the ggpairs function will automatically calculate some correlation values and put them on the plot as well. I’ve always done these using old school pairs and I thought it would be fun to look at one using the ggplot version. One big advantage is how it automatically uses the facet_grid framework to make the plots and include special plots along the diagonal. You could do the same thing with pairs, but it was more work.\nA few notes about this plot:\n\nThe density plots for the sovereign queens are “spikey” (for lack of a better term). There are just so few sovereign queens. Furthermore the age range at marriage is broad enough that this distribution is just not smooth. Until recently, the succession rules heavily favored male heirs. With the new rules recently adapted, maybe in another millenium or two, there will be enough sovereign queens that we can start approaching gender parity in this area.\nAlso, even though the sovereign queens were the monarch, they also were (in general) younger than their consort, while the opposite is true for the kings."
  },
  {
    "objectID": "posts/tt20240819/englishMonarchy.html#the-model-that-wasnt-quite-there",
    "href": "posts/tt20240819/englishMonarchy.html#the-model-that-wasnt-quite-there",
    "title": "English Monarchy",
    "section": "The model that wasn’t quite there",
    "text": "The model that wasn’t quite there\n\nI like making different kinds of models. I’m wondering if we can build a model for sovereign age at marriage given inputs like gender and year. I didn’t do it here, in part, because I think that there is other work to be done. In particular, how long a sovereign has been in power would be useful data to gather, as well as figuring out how to deal with multiple marriages such as the case of Henry VIII.\n\n\n\n# A tibble: 6 × 5\n  sovereign_name sovereign_age consort_name        consort_age year_of_marriage\n  &lt;chr&gt;          &lt;chr&gt;         &lt;chr&gt;               &lt;chr&gt;       &lt;chr&gt;           \n1 Henry VIII     18            Catherine of Aragon 24          1509            \n2 Henry VIII     42            Anne Boleyn         32          1533            \n3 Henry VIII     45            Jane Seymour        28          1536            \n4 Henry VIII     49            Anne of Cleves      25          1540            \n5 Henry VIII     49            Catherine Howard    19          1540            \n6 Henry VIII     52            Catherine Parr      31          1543"
  },
  {
    "objectID": "posts/tt20240819/englishMonarchy.html#ew",
    "href": "posts/tt20240819/englishMonarchy.html#ew",
    "title": "English Monarchy",
    "section": "Ew",
    "text": "Ew\nI thought about ignoring this, but 2024 me simply cannot. I know it was a different time. I am simply upset that a society allowed this sort of predatory behavior under the guise of a self-proclaimed and self-serving monarch declaring his divine right as well as the nobility that enabled and enforced this behavior in service of the accumulation of power. So I’m creating a table and calling it my “Hall of Shame.” I’m not sure that age difference in a relationship is the single best way for building this group, but that’s the data that I have in front of me to use.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHall of Shame\n\n\nPerpetraitor\nVictim\nΔ\nYear\n\n\nName\nAge\nName\nAge\n\n\n\n\nEdward I\n60\nMargaret of France\n20\n40\n1299\n\n\nÆthelwulf\n50\nJudith of Flanders\n12\n38\n856\n\n\nHenry I\n53\nAdeliza of Louvain\n18\n35\n1121\n\n\nHenry VIII\n49\nCatherine Howard\n19\n30\n1540\n\n\nWilliam IV\n53\nAdelaide of Saxe-Meiningen\n26\n27\n1818\n\n\nJames II\n40\nMary of Modena\n15\n25\n1673\n\n\n\n#TidyTuesday • Sources: Ian Visits and f. hull dataset"
  },
  {
    "objectID": "readme.html",
    "href": "readme.html",
    "title": "Steven Wolf - Data World",
    "section": "",
    "text": "This is the weblog so I can do some #TidyTuesday activities and play with Quarto.\nGo to the webpage here: datawolf04.github.io"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m a physicist by training, but a Data Scientist by practice. I’ve been coding things in R and Python for several years, and I created this website as a place to put my fun projects, organize my thoughts, and do other data science related things. You can find me on LinkedIn, Github, and Google Scholar (links below). You can find me active within different Data Science spheres including #TidyTuesday and the Data Science Learning Community. When I’m not at work, or playing with data, you can find me outside, I love to go hiking and kayaking."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\n\nMichigan State University | East Lansing, MI  Ph.D. in Physics 2006-2012\nDartmouth College | Hanover, NH  M.S. in Physics 2003-2005\nValparaiso University | Valparaiso, IN  B.S. in Physics and Mathematics 1999-2003"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About",
    "section": "Experience",
    "text": "Experience\n\nEast Carolina University | Assistant Professor of Physics | August 2015 -present\nTexas State University | Physics Lecturer | August 2014 - August 2015\nMichigan State University | Research Associate | August 2012 - August 2014"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Steven Wolf - Data World",
    "section": "",
    "text": "Stack Overflow Annual Developer Survey\n\n\n\n\n\n\ntidyTuesday\n\n\nR\n\n\n\nWhere are Stack Overflow users working from? Let’s find out.\n\n\n\n\n\nSep 5, 2024\n\n\nSteven Wolf\n\n\n\n\n\n\n\n\n\n\n\n\nEnglish Monarchy\n\n\n\n\n\n\ntidyTuesday\n\n\nR\n\n\n\n\n\n\n\n\n\nAug 20, 2024\n\n\nSteven Wolf\n\n\n\n\n\n\n\n\n\n\n\n\nWorld’s Fairs\n\n\n\n\n\n\ntidyTuesday\n\n\nR\n\n\nmaps\n\n\n\nI’ve always wanted to play with making a map-based visualization\n\n\n\n\n\nAug 14, 2024\n\n\nSteven Wolf\n\n\n\n\n\n\n\n\n\n\n\n\nScraping Weather Data-Proof of concept\n\n\n\n\n\n\nweather\n\n\npython\n\n\nweb-scraping\n\n\n\nScraping weather data near Zebulon, NC\n\n\n\n\n\nAug 8, 2024\n\n\nSteven Wolf\n\n\n\n\n\n\n\n\n\n\n\n\nOlympic countries: Fun facts and visualizations\n\n\n\n\n\n\ntidyTuesday\n\n\nR\n\n\n\nCountries by participation and medal count\n\n\n\n\n\nAug 7, 2024\n\n\nSteven Wolf\n\n\n\n\n\n\n\n\n\n\n\n\nSummer Movies\n\n\n\n\n\n\ntidyTuesday\n\n\nR\n\n\n\nAnalyzing the IMDB for movies with ‘summer’ in the title\n\n\n\n\n\nAug 2, 2024\n\n\nSteven Wolf\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/summerMovies/summerMovies.html",
    "href": "posts/summerMovies/summerMovies.html",
    "title": "Summer Movies",
    "section": "",
    "text": "This week we’re exploring “summer” movies: movies with summer in their title!\nThe data this week comes from the Internet Movie Database.\nlibrary(tidyverse)\nlibrary(ggtext)\nlibrary(showtext)\nlibrary(janitor)\nlibrary(ggdist)\nlibrary(ggrepel)\nlibrary(scales)\nlibrary(paletteer)\n\nload('summerMovie.RData')"
  },
  {
    "objectID": "posts/summerMovies/summerMovies.html#popularity-of-movie-by-year",
    "href": "posts/summerMovies/summerMovies.html#popularity-of-movie-by-year",
    "title": "Summer Movies",
    "section": "Popularity of movie by year",
    "text": "Popularity of movie by year\nIn some sense how well-known or popular a movie is should depend on the year. I’m guessing that very old movies may not have as many votes in the IMDB system. This could also bias the ratings\n\nggplot(summer_movies, aes(x=year,y=num_votes)) +\n  geom_point(color=oneCol) + \n  labs(\n    x = \"Year movie released\",\n    y = \"Number of ratings\",\n    caption = caption_text\n  ) + theme_simple()\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nI can definitely see that there is a volume bias towards newer movies. Let’s explore if this shows up in the ratings.\n\nggplot(summer_movies, aes(x=num_votes, y=average_rating, color=year)) +\n  geom_point() + \n  scale_x_log10(breaks = trans_breaks(\"log10\", function(x) 10^x),\n                labels = trans_format(\"log10\", math_format(10^.x))) +\n  scale_y_continuous(limits = c(0,10)) + \n  scale_color_paletteer_c(col_pal_cont) +\n  labs(\n    x = \"Number of ratings\",\n    y = \"Average rating\",\n    color = \"Year\",\n    caption = caption_text\n  ) + theme_simple()\n\n\n\n\n\n\n\n\nI’d say that there are no truly universally bad summer movies (for example with everyone scoring it a 0 or 1 out of 10). Despite the recency bias in the rating volume, it would seem that movies can be rated poorly or highly regardless of the year the movie was released. It also seems like there is a “reversion to the mean” effect for movies that have more votes, although this could simply be the result of sparse data. By “reversion to the mean” I am referring to the vaguely triangular shape of the blob of points, suggesting that as a movie is rated more and more, the diversity of opinion forces the mean rating to tend away from extreme values."
  },
  {
    "objectID": "posts/summerMovies/summerMovies.html#re-imagining-other-work",
    "href": "posts/summerMovies/summerMovies.html#re-imagining-other-work",
    "title": "Summer Movies",
    "section": "Re-imagining other work",
    "text": "Re-imagining other work\nAs I’m doing this a bit late in the game, I can take advantage of the work some others have done. I saw this plot, and thought it would be good to replicate here, with a subtle twist.\n\nRather than plotting mean values for each genre, I thought I’d make a box plot instead of plotting the average rating. So I will tidy up the data. Furthermore, since I’m creating a box plot, I’m going to remove genres with fewer than \\(N=5\\) ratings.\n\nglobalMedian = median(summer_movies$average_rating, na.rm = TRUE)\ngDat &lt;- summer_movies |&gt; separate_longer_delim(cols = genres, delim = \",\") |&gt;\n  group_by(genres) \n\nnewGenre &lt;-  gDat |&gt;\n  summarise(\n    count = n(),\n    rating = median(average_rating, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\ndata4plot = full_join(newGenre,gDat,by=\"genres\") |&gt;\n  mutate(\n      genre_count = str_glue(\"{genres} ({count})\"),\n      genre_count = fct_reorder(genre_count, rating, .desc = FALSE),\n      highlight = ifelse(rating &gt;= globalMedian, \"yes\",\"no\")\n      ) |&gt;\n  filter(count&gt;=5)\n\nAnd finally, the plot.\n\nggplot(data4plot, aes(x=average_rating, y=genre_count, fill=highlight)) +\n  geom_vline(\n    xintercept = globalMedian,\n    linewidth = .5,\n    color = 'gray'\n    ) +\n  geom_boxplot() +\n  labs(\n    x = \"Rating\",\n    y = \"Movie Genre (count)\",\n    caption = caption_text\n  ) +\n  scale_x_continuous(breaks = seq(2,10,by=1), limits=c(2,10)) +\n  scale_y_discrete() +\n  scale_fill_paletteer_d(col_pal_dis) +\n  coord_cartesian(clip='off') + theme_catY()"
  },
  {
    "objectID": "posts/summerMovies/summerMovies.html#final-notes",
    "href": "posts/summerMovies/summerMovies.html#final-notes",
    "title": "Summer Movies",
    "section": "Final notes",
    "text": "Final notes\nI just found out about #TidyTuesday, just this week, and I want to participate. So the purpose of this post is mostly to get a blog going, and I hope to update approximately monthly. Now that this post is written, we’ll see if I can get this onto Github Pages. 😄\n[Edit: Phew! That worked!]"
  },
  {
    "objectID": "posts/weatherScrape240808/weatherScrape.html",
    "href": "posts/weatherScrape240808/weatherScrape.html",
    "title": "Scraping Weather Data-Proof of concept",
    "section": "",
    "text": "My friend, Ben Leese, was telling me about his most recent project. He has a passion for going through old naturalist’s notebooks and pulling out data from the depths of that analog mess and bringing it into the digital world. He was talking to me about how weather could impact different bird behaviors. But he only had binary weather data (Yes, it rained/No, it didn’t rain). Furthermore, it was from Raleigh, NC rather than Zebulon, NC. While these places are close on the map, weather is even more local than politics. So I said that I’d try to find some better weather data for him.\nI found this helpful python script for scraping the weather data from Weather Underground, which I will adapt to my purpose.\n\nimport time\nimport sys\n\nimport numpy as np\nimport pandas as pd\nfrom bs4 import BeautifulSoup as BS\nfrom selenium import webdriver\nfrom great_tables import GT\nfrom scrape_wunderground import *\n\nI will scrape weather from the BreakingWind station with code KNCZEBUL74 on 8/1/2024, as I am pretty sure this is the closest station to the desired location.\n\nstation_id = \"KNCZEBUL74\"\ndate_id = \"2024-08-01\"\n\n(\n  GT(scrape_wunderground(station_id,date_id).head(20))\n    .tab_options(\n      column_labels_background_color = \"#3B3A3EFF\",\n  )\n)\n\n\n\n\n\n\n\ntimestamps\nTemperature\nDew Point\nHumidity\nWind Speed\nWind Gust\nPressure\nPrecip. Rate\nPrecip. Accum.\n\n\n\n\n2024-08-01 12:04 AM\n75.0\n72.0\n92.0\n0.0\n0.0\n29.92\n0.0\n0.0\n\n\n2024-08-01 12:09 AM\n75.0\n72.0\n92.0\n0.0\n0.0\n29.92\n0.0\n0.0\n\n\n2024-08-01 12:14 AM\n75.0\n72.0\n92.0\n0.0\n0.0\n29.92\n0.0\n0.0\n\n\n2024-08-01 12:19 AM\n74.8\n72.0\n91.0\n0.0\n0.0\n29.92\n0.0\n0.0\n\n\n2024-08-01 12:24 AM\n74.8\n72.0\n91.0\n0.0\n0.0\n29.93\n0.0\n0.0\n\n\n2024-08-01 12:29 AM\n74.8\n72.0\n91.0\n0.0\n0.0\n29.93\n0.0\n0.0\n\n\n2024-08-01 12:34 AM\n74.7\n72.0\n92.0\n0.0\n0.0\n29.93\n0.0\n0.0\n\n\n2024-08-01 12:39 AM\n74.7\n72.0\n92.0\n0.0\n0.0\n29.93\n0.0\n0.0\n\n\n2024-08-01 12:44 AM\n74.6\n72.0\n92.0\n0.0\n0.0\n29.93\n0.0\n0.0\n\n\n2024-08-01 12:49 AM\n74.5\n72.6\n93.0\n0.0\n0.0\n29.93\n0.0\n0.0\n\n\n2024-08-01 12:54 AM\n74.5\n73.0\n93.0\n0.0\n0.0\n29.93\n0.0\n0.0\n\n\n2024-08-01 12:59 AM\n74.7\n73.0\n93.0\n0.0\n0.0\n29.93\n0.0\n0.0\n\n\n2024-08-01 1:04 AM\n74.7\n73.0\n93.0\n0.0\n0.0\n29.92\n0.0\n0.0\n\n\n2024-08-01 1:09 AM\n74.7\n72.3\n92.0\n0.0\n0.0\n29.92\n0.0\n0.0\n\n\n2024-08-01 1:14 AM\n74.7\n72.2\n92.0\n0.0\n0.0\n29.92\n0.0\n0.0\n\n\n2024-08-01 1:19 AM\n74.6\n72.0\n92.0\n0.0\n0.0\n29.92\n0.0\n0.0\n\n\n2024-08-01 1:24 AM\n74.5\n72.0\n92.0\n0.0\n0.0\n29.92\n0.0\n0.0\n\n\n2024-08-01 1:29 AM\n74.5\n72.0\n92.0\n0.0\n0.0\n29.92\n0.0\n0.0\n\n\n2024-08-01 1:34 AM\n74.5\n72.9\n93.0\n0.0\n0.0\n29.92\n0.0\n0.0\n\n\n2024-08-01 1:39 AM\n74.5\n73.0\n93.0\n0.0\n0.0\n29.92\n0.0\n0.0\n\n\n\n\n\n\n        \n\n\nAnd it works! That being said, I’ll have to find a different weather station because this one seems somewhat new. There is only partial data for 5/29/2024.\n\ndate_id = \"2024-05-29\"\n(\n  GT(scrape_wunderground(station_id,date_id).head(20))\n    .tab_options(\n      column_labels_background_color = \"#3B3A3EFF\",\n  )\n)\n\n\n\n\n\n\n\ntimestamps\nTemperature\nDew Point\nHumidity\nWind Speed\nWind Gust\nPressure\nPrecip. Rate\nPrecip. Accum.\n\n\n\n\n2024-05-29 10:34 PM\n71.2\n51.2\n50.0\n0.2\n0.2\n29.93\n0.0\n0.0\n\n\n2024-05-29 10:39 PM\n71.7\n53.9\n54.0\n1.4\n9.0\n29.94\n0.1\n0.1\n\n\n2024-05-29 10:44 PM\n73.1\n55.1\n53.0\n0.8\n10.0\n29.94\n0.1\n0.1\n\n\n2024-05-29 10:49 PM\n72.8\n54.9\n53.0\n0.0\n10.0\n29.94\n0.1\n0.1\n\n\n2024-05-29 10:54 PM\n72.4\n53.5\n52.0\n0.0\n10.0\n29.94\n0.1\n0.1\n\n\n2024-05-29 10:59 PM\n72.2\n53.0\n52.0\n0.0\n10.0\n29.94\n0.1\n0.1\n\n\n2024-05-29 11:04 PM\n72.1\n53.0\n52.0\n0.0\n10.0\n29.95\n0.1\n0.1\n\n\n2024-05-29 11:09 PM\n72.0\n53.0\n52.0\n0.0\n10.0\n29.95\n0.1\n0.1\n\n\n2024-05-29 11:14 PM\n71.9\n52.1\n51.0\n0.0\n10.0\n29.95\n0.1\n0.1\n\n\n2024-05-29 11:19 PM\n71.8\n51.8\n51.0\n0.0\n10.0\n29.95\n0.1\n0.1\n\n\n2024-05-29 11:24 PM\n71.6\n51.6\n51.0\n0.0\n10.0\n29.95\n0.1\n0.1\n\n\n2024-05-29 11:29 PM\n71.6\n51.0\n50.0\n0.0\n10.0\n29.95\n0.1\n0.1\n\n\n2024-05-29 11:34 PM\n71.4\n51.0\n49.0\n0.0\n10.0\n29.95\n0.1\n0.1\n\n\n2024-05-29 11:39 PM\n71.3\n51.0\n49.0\n0.0\n8.0\n29.95\n0.1\n0.1\n\n\n2024-05-29 11:44 PM\n71.2\n50.1\n48.0\n0.0\n2.0\n29.95\n0.0\n0.1\n\n\n2024-05-29 11:49 PM\n71.1\n50.0\n47.0\n0.0\n1.3\n29.95\n0.0\n0.1\n\n\n2024-05-29 11:54 PM\n71.1\n50.0\n47.0\n0.0\n0.0\n29.94\n0.0\n0.1\n\n\n2024-05-29 11:59 PM\n70.9\n50.0\n47.0\n0.0\n0.0\n29.94\n0.0\n0.1\n\n\n\n\n\n\n        \n\n\nAnd 5/28/2024 has no data.\n\ndate_id = \"2024-05-28\"\n(\n  GT(scrape_wunderground(station_id,date_id).head(20))\n    .tab_options(\n      column_labels_background_color = \"#3B3A3EFF\",\n  )\n)\n\n\n\n\n\n\n\ntimestamps\nTemperature\nDew Point\nHumidity\nWind Speed\nWind Gust\nPressure\nPrecip. Rate\nPrecip. Accum.\n\n\n\n\n\n\n\n\n        \n\n\nI will have to find a different nearby weather station for this purpose of finding Ben some weather data for the time period he is interested in (mid 1970s). And, once I succeed at that, I will have to aggregate the 5-minute data to daily data. But once that’s done, my friend should have more than enough weather data to help him with his model."
  }
]